0:00:04.070,0:00:07.059
В останньому відео я виклав структуру нейронної мережі

0:00:07.160,0:00:10.089
Я дам тут короткий підсумок лише для того, щоб він був свіжим у нашому розумі

0:00:10.089,0:00:15.368
І тоді у мене є дві основні цілі цього відео. Перший - ввести ідею градієнтного спуску,

0:00:15.650,0:00:18.219
що лежить в основі не тільки того, як навчаються нейронні мережі,

0:00:18.220,0:00:20.439
але як працює багато інших машинного навчання

0:00:20.660,0:00:24.609
Потім після цього ми ще трохи розберемося, як працює ця конкретна мережа

0:00:24.609,0:00:27.758
І те, що ці приховані шари нейронів насправді шукають

0:00:28.999,0:00:33.489
Нагадування - наша мета - класичний приклад розпізнавання від руки цифр

0:00:34.129,0:00:36.129
привіт світ нейронних мереж

0:00:36.500,0:00:43.090
ці цифри відображаються на сітці 28 на 28 пікселів кожного пікселя з деяким значенням масштабу сірого між 0 і 1

0:00:43.610,0:00:46.089
це те, що визначає активацію

0:00:46.850,0:00:50.199
784 нейронів у вхідному шарі мережі та

0:00:50.840,0:00:55.719
Тоді активація кожного нейрона в наступних шарах базується на зваженій сумі

0:00:56.000,0:01:00.639
Усі активації в попередньому шарі плюс якесь спеціальне число, яке називається зміщенням

0:01:01.699,0:01:06.338
тоді ви складаєте цю суму за допомогою якоїсь іншої функції, наприклад, сигмоподібної сцизифікації або

0:01:06.400,0:01:08.769
a ReLu так, як я пройшов минуле відео

0:01:09.110,0:01:15.729
В цілому, враховуючи дещо довільний вибір двох прихованих шарів, що мають 16 нейронів у кожній мережі

0:01:16.579,0:01:24.159
13000 ваг і ухилів, які ми можемо регулювати, і саме ці значення визначають, що саме робить мережа, яку ви знаєте

0:01:24.799,0:01:28.328
Тоді що ми маємо на увазі, коли говоримо, що ця мережа класифікує задану цифру

0:01:28.329,0:01:33.429
Хіба що найяскравіший з цих 10 нейронів у заключному шарі відповідає цій цифрі

0:01:33.950,0:01:38.589
І пам’ятайте, що мотивація, яку ми мали тут на увазі для шаруватої структури, - це можливо

0:01:38.780,0:01:44.680
Другий шар може підхоплюватися по краях, а третій шар може підбиратись за візерунками, такими як петлі та лінії

0:01:44.930,0:01:48.729
І останній міг просто зібрати ці шаблони, щоб розпізнати цифри

0:01:49.369,0:01:52.029
Отже тут ми дізнаємось, як навчається мережа

0:01:52.399,0:01:57.099
Ми хочемо - це алгоритм, за допомогою якого ви можете показати цій мережі цілу купу навчальних даних

0:01:57.229,0:02:03.669
який випускається у вигляді кучі різних зображень рукописних цифр разом із мітками для того, що вони мають бути і

0:02:03.890,0:02:05.659
Це налаштує їх

0:02:05.659,0:02:09.789
13000 ваг і ухилів, щоб поліпшити його результативність щодо даних тренувань

0:02:10.730,0:02:13.569
Сподіваємось, ця шарувата структура означатиме те, що вона вивчить

0:02:14.269,0:02:16.719
узагальнює зображення, що перевищують дані тренувань

0:02:16.720,0:02:20.289
І те, як ми тестуємо, це те, що ви тренуєте мережу

0:02:20.290,0:02:26.560
Ви показуєте це більш міче тета, що її ніколи не бачили, і ви бачите, наскільки точно класифікує ці нові зображення

0:02:31.040,0:02:37.000
На щастя для нас, і що робить це таким загальним прикладом для початку, це те, що хороші люди, що стоять за базою MNIST, мають

0:02:37.000,0:02:44.289
зібрати колекцію з десятків тисяч рукописних цифрових зображень, кожне з яких позначено цифрами, якими вони повинні бути і

0:02:44.720,0:02:49.539
Це провокативно, оскільки описувати машину як навчання, як тільки ви насправді бачите, як вона працює

0:02:49.540,0:02:55.359
Це відчуває себе набагато менше, як деякий шалений науково-фантастичний задум, і набагато більше схоже на вправу на числення

0:02:55.390,0:02:59.589
Я маю на увазі, це в основному зводиться до пошуку мінімуму певної функції

0:03:01.519,0:03:05.199
Запам’ятайте, що концептуально ми думаємо, що кожен нейрон пов'язаний

0:03:05.390,0:03:12.309
для всіх нейронів попереднього шару, а ваги у зваженій сумі, що визначають його активацію, схожі на

0:03:12.440,0:03:14.060
сильні сторони цих зв'язків

0:03:14.060,0:03:20.440
І упередженість є певним свідченням того, чи є цей нейрон активним чи неактивним і починати щось

0:03:20.440,0:03:26.919
Ми просто ініціалізуємо всі ці ваги та ухили абсолютно випадковим чином, що не потрібно говорити, що ця мережа буде виконуватись

0:03:26.919,0:03:33.759
досить жахливо на даному прикладі тренувань, оскільки це просто робиться щось випадкове, наприклад, ви подаєте це зображення 3-х та

0:03:33.760,0:03:35.799
Вихідний шар він просто схожий на безлад

0:03:36.349,0:03:42.518
Отже, що ви робите, це ви визначаєте функцію витрат як спосіб сказати комп'ютеру: "Не поганий комп'ютер!

0:03:42.739,0:03:50.529
Цей вихід повинен мати активацію, яка для більшості нейронів дорівнює нулю, але одна для цього нейрона, що ви мені дали, - це зовсім сміття "

0:03:51.260,0:03:56.530
Сказати, що трохи більш математично те, що ви робите, - це скласти квадрати відмінностей між

0:03:56.720,0:04:01.419
кожне з цих активацій виведення сміття та значення, яке ви хочете, щоб вони мали і

0:04:01.489,0:04:04.599
Це те, що ми назвемо ціною одного прикладу навчання

0:04:05.599,0:04:10.749
Зауважте, що ця сума невелика, коли мережа впевнено класифікує зображення правильно

0:04:12.199,0:04:15.639
Але це велике місце, коли мережа здається, що насправді не знає, що робить

0:04:18.330,0:04:25.249
Тож, то, що ви робите, - це врахувати середню вартість за всі десятки тисяч прикладів навчання, що є у вашому розпорядженні

0:04:27.060,0:04:34.310
Ця середня вартість є нашим показником того, наскільки вразлива мережа та як погано себе почуває комп’ютер, і це справа складна

0:04:34.830,0:04:38.960
Згадайте, як саме мережа була в основному функцією, яку приймає

0:04:39.540,0:04:45.890
784 числа вводить значення пікселів і виділяє десять чисел як вихід і в певному сенсі

0:04:45.890,0:04:48.770
Це параметризується усіма цими вагами та ухилами

0:04:49.140,0:04:54.020
Хоча функція витрат є шаром складності, окрім того, що вона бере свій внесок

0:04:54.450,0:05:02.059
ці тринадцять тисяч ваг і ухилів, і це виписує єдине число, що описує, наскільки погані ці ваги і ухили і

0:05:02.340,0:05:08.749
Спосіб її визначення залежить від поведінки мережі у всіх десятках тисяч одиниць навчальних даних

0:05:09.150,0:05:11.150
Про це багато про що подумати

0:05:12.000,0:05:15.619
Але просто сказати комп’ютеру, що це за хитра робота, це робити не дуже корисно

0:05:15.900,0:05:19.819
Ви хочете сказати, як змінити ваги та ухили, щоб вони стали кращими?

0:05:20.820,0:05:25.129
Щоб зробити це легше, ніж намагатися уявити функцію з 13000 входами

0:05:25.130,0:05:30.409
Просто уявіть просту функцію, яка має одне число як вхідне і одне число як вихідне

0:05:30.960,0:05:34.999
Як ви знайдете вхід, який мінімізує значення цієї функції?

0:05:36.270,0:05:40.039
Студенти з розрахунку знають, що іноді ви можете зрозуміти цей мінімум чітко

0:05:40.260,0:05:43.879
Але це не завжди можливо для дійсно складних функцій

0:05:44.310,0:05:52.160
Звичайно, це не в тринадцяти тисячах вхідних версій цієї ситуації для нашого шаленого складного функціонування нейронної мережі

0:05:52.350,0:05:59.029
Більш гнучка тактика полягає в тому, щоб почати з будь-якого старого вводу і визначити, в якому напрямку ви повинні крокувати, щоб зробити цей результат нижчим

0:06:00.120,0:06:03.710
Зокрема, якщо ви можете з'ясувати нахил функції, де ви знаходитесь

0:06:04.020,0:06:09.619
Потім перемістіть вліво, якщо цей нахил позитивний, і змістіть ввід праворуч, якщо цей нахил негативний

0:06:12.130,0:06:16.799
Якщо ви робите це неодноразово в кожній точці, перевіряйте новий схил і робите відповідний крок

0:06:16.800,0:06:20.039
ти підійдеш до якогось локального мінімуму функції та

0:06:20.280,0:06:24.080
зображення, яке ви можете мати на увазі тут, - це кулька, що котиться вниз по пагорбі і

0:06:24.400,0:06:30.900
Зверніть увагу навіть на цю дійсно спрощену функцію одного входу, існує багато можливих долин

0:06:31.540,0:06:36.220
Залежно від того, з якого випадкового вводу ви починаєте, і немає гарантії, що локальний мінімум

0:06:36.580,0:06:39.040
Ви приземляєтесь, це буде найменшим можливим значенням функції витрат

0:06:39.610,0:06:44.009
Це також перейде до нашої справи з нейронної мережі, і я також хочу, щоб ви це помітили

0:06:44.010,0:06:47.190
Як, якщо ви зробите розміри кроків пропорційними схилу

0:06:47.620,0:06:54.540
Тоді, коли нахил вирівнюється до мінімуму, ваші кроки стають все меншими та меншими, і такий спосіб допомагає вам від перенапруги

0:06:55.720,0:07:00.449
Трохи збільшивши складність, уявіть собі функцію з двома входами та одним виходом

0:07:01.120,0:07:07.739
Ви можете подумати про вхідний простір як площину XY, а функцію витрат - як про поверхню над ним

0:07:08.230,0:07:15.060
Тепер замість того, щоб запитувати про нахил функції, ви повинні запитати, в якому напрямку слід крокувати в цьому вхідному просторі?

0:07:15.310,0:07:22.440
Так, щоб швидше зменшити вихід функції, іншими словами. Який напрямок вниз?

0:07:22.440,0:07:25.379
І знову корисно подумати про кулю, що котиться вниз по цій горі

0:07:26.260,0:07:34.080
Ті, хто з вас знайомий з багатовимірним численням, знають, що градієнт функції дає вам напрям найкрутішого сходження.

0:07:34.750,0:07:38.459
В основному, в якому напрямку ви б ходили, щоб швидше збільшити функцію

0:07:39.100,0:07:46.439
Звичайно, досить негативне враження градієнта дає тобі напрямок кроку, який швидше зменшує функцію та

0:07:47.020,0:07:53.400
Навіть більше, ніж довжина цього градієнтного вектора насправді є свідченням того, наскільки крутий цей крутий схил

0:07:54.130,0:07:56.280
Тепер, якщо ви незнайомі з багатовимірним обчисленням

0:07:56.280,0:08:00.239
І ви хочете дізнатися більше, ознайомтеся з деякою роботою, яку я зробив для Академії Хана з цієї теми

0:08:00.910,0:08:03.779
Чесно кажучи, хоча все, що має значення для вас і мене зараз

0:08:03.780,0:08:09.419
Чи в принципі існує спосіб обчислити цей вектор. Цей вектор, який говорить вам, що

0:08:09.520,0:08:15.900
Напрямок на спуск - і наскільки крутий він, ви будете добре, якщо це все, що знаєте, і ви не тверді в деталях

0:08:16.790,0:08:24.580
тому що якщо ви можете отримати, що алгоритм мінімізації функції полягає в обчисленні цього напрямку градієнта, тоді зробіть невеликий крок вниз і

0:08:24.740,0:08:26.740
Просто повторіть це знову і знову

0:08:27.800,0:08:34.600
Це та сама основна ідея для функції, яка має 13000 входів замість двох входів, як уявити, організувати всіх

0:08:35.330,0:08:39.400
13000 ваг і ухилів нашої мережі в гігантський вектор стовпців

0:08:39.680,0:08:43.870
Від'ємний градієнт функції витрат - це просто вектор

0:08:43.880,0:08:49.299
Це якийсь напрямок всередині цього шалено величезного вхідного простору

0:08:49.400,0:08:55.030
натискання на всі ці числа призведе до найшвидшого зниження функції витрат і

0:08:55.460,0:08:58.150
звичайно з нашою спеціально розробленою функцією витрат

0:08:58.580,0:09:04.900
Зміна ваг та ухилів до зменшення означає, що це робить висновок мережі на кожній частині даних про навчання

0:09:05.180,0:09:10.599
Виглядають менше як випадковий масив з десяти значень і більше нагадують фактичне рішення, яке ми хочемо прийняти

0:09:11.030,0:09:16.030
Важливо пам’ятати, що ця функція витрат передбачає в середньому за всі дані про навчання

0:09:16.370,0:09:20.590
Тож якщо ви мінімізуєте це, це означає, що це покращить ефективність усіх цих зразків

0:09:23.780,0:09:30.849
Алгоритм ефективного обчислення цього градієнта, який є серцем того, як навчається нейронна мережа, називається зворотним поширенням.

0:09:31.190,0:09:34.690
І саме про це я буду говорити про наступне відео

0:09:34.690,0:09:36.690
Там я дуже хочу витратити час на прогулянку

0:09:36.830,0:09:41.439
Що саме відбувається з кожною вагою і кожним ухилом для даної частини тренувальних даних?

0:09:41.810,0:09:46.960
Намагаючись дати інтуїтивно зрозуміти, що відбувається поза цією групою відповідних обчислень та формул

0:09:47.510,0:09:52.179
Саме тут зараз головне. Я хочу, щоб ви знали незалежно від деталей реалізації

0:09:52.180,0:09:58.479
це те, що ми маємо на увазі, коли ми говоримо про мережеве навчання, це те, що це просто мінімізувати функції витрат і

0:09:58.940,0:10:04.479
Зауважте, одним із наслідків цього є те, що для цієї функції витрат важливо мати хороший плавний вихід

0:10:04.480,0:10:07.810
Щоб ми могли знайти місцевий мінімум, зробивши невеликі кроки вниз

0:10:08.810,0:10:10.520
Ось чому, до речі

0:10:10.520,0:10:16.749
Штучні нейрони мають активізацію безперервно, а не просто бути активними або неактивними бінарними способами

0:10:16.750,0:10:18.750
якщо так є біологічні нейрони

0:10:19.940,0:10:26.770
Цей процес багаторазового натискання на вхід функції деяким кратним негативним градієнтом називається градієнтним спуском

0:10:26.930,0:10:32.380
Це спосіб сходити до деякого локального мінімуму функції витрат, в основному долини на цьому графіку

0:10:32.930,0:10:38.890
Я все ще показую картину функції з двома входами, звичайно, тому, що натискає на тринадцять тисяч вхідних розмірів

0:10:38.890,0:10:44.049
Простір трохи важко обернути розумом, але насправді є приємний непросторовий спосіб подумати над цим

0:10:44.630,0:10:51.340
Кожен компонент негативного градієнта повідомляє нам дві речі, знак звичайно підказує нам, чи відповідні

0:10:51.830,0:10:59.139
Компонент вхідного вектора повинен бути висунутим вгору або вниз, але важливо відносні величини всіх цих компонентів

0:10:59.840,0:11:02.530
Начебто говорить про те, які зміни важливіші

0:11:05.150,0:11:09.340
Ви бачите, що в нашій мережі пристосування до однієї з ваг може бути набагато більшим

0:11:09.710,0:11:12.939
вплив на функцію витрат, ніж пристосування до якоїсь іншої ваги

0:11:14.450,0:11:17.950
Деякі з цих зв'язків мають більше значення для наших даних про навчання

0:11:18.920,0:11:22.690
Таким чином, ви можете думати про цей градієнтний вектор нашого розуму

0:11:22.690,0:11:27.999
Функція масивної вартості полягає в тому, що вона кодує відносну важливість кожної ваги та зміщення

0:11:28.250,0:11:32.200
Ось яка з цих змін несе найбільше грошей на ваш долар

0:11:33.560,0:11:36.460
Це дійсно просто інший спосіб думати про напрямок

0:11:36.860,0:11:41.290
Візьмемо простіший приклад, якщо у вас є якась функція з двома змінними як вхід і ви

0:11:41.690,0:11:46.540
Обчисліть, що його градієнт у певній точці виходить як (3,1)

0:11:47.420,0:11:51.670
Тоді, з одного боку, ви можете інтерпретувати це так, як кажучи, що коли ви стоїте біля цього входу

0:11:52.070,0:11:55.150
переміщення по цьому напрямку збільшує функцію найшвидше

0:11:55.460,0:12:02.229
Коли ви графікуєте функцію над площиною вхідних точок, цей вектор - це те, що дає вам прямий напрямок в гору

0:12:02.600,0:12:06.580
Але ще один спосіб прочитати - сказати, що змінюється ця перша змінна

0:12:06.740,0:12:13.390
Мають у три рази важливіше значення, як зміни у другій змінній, що принаймні в околицях відповідного вводу

0:12:13.520,0:12:16.689
Натискання значення x приносить набагато більше ударів за ваш долар

0:12:19.310,0:12:19.930
Гаразд

0:12:19.930,0:12:24.940
Давайте зменшимо масштаб і підведемо підсумки, де ми зараз перебуваємо в цій мережі, саме ця функція

0:12:25.400,0:12:29.859
784 входів і 10 виходів, визначених у перерахунку на всі ці зважені суми

0:12:30.350,0:12:34.780
функція витрат - це складний шар, крім того, який він займає

0:12:35.120,0:12:41.870
13000 ваг і ухилів як вхідних даних і випльовує єдину міру паршивості на основі прикладів тренувань і

0:12:42.180,0:12:47.930
Градієнт функції витрат - це ще один рівень складності, який він нам каже

0:12:47.930,0:12:53.839
Що підштовхує всі ці ваги та ухили, викликає найшвидшу зміну значення функції витрат

0:12:53.970,0:12:57.680
Що ви можете трактувати, говорить про те, які зміни, які ваги мають найбільше значення

0:13:02.550,0:13:09.289
Тож, коли ви ініціалізуєте мережу випадковими вагами та ухилами та коригуєте їх багато разів, виходячи з цього процесу спуску градієнта

0:13:09.420,0:13:12.949
Наскільки добре він насправді виконує зображення, яких раніше ніколи не бачив?

0:13:13.680,0:13:19.609
Добре той, який я описав тут, з двома прихованими шарами шістнадцяти нейронів, кожен з яких обраний здебільшого з естетичних причин

0:13:20.579,0:13:26.089
добре, це непогано, що він класифікує близько 96 відсотків нових зображень, які він бачить правильно і

0:13:26.759,0:13:32.239
Чесно кажучи, якщо ви подивитеся на кілька прикладів, які це заплутує, ви начебто змушені трохи зменшити його

0:13:35.759,0:13:39.079
Тепер, якщо ви пограєте зі структурою прихованого шару і зробите кілька перетворень

0:13:39.079,0:13:43.698
Ви можете отримати це до 98%, і це дуже добре. Це не найкраще

0:13:43.740,0:13:48.409
Ви, звичайно, можете досягти кращих показників, ставши більш досконалими, ніж ця звичайна ванільна мережа

0:13:48.569,0:13:52.669
Але враховуючи, наскільки страшною є початкове завдання, я просто думаю, що є щось?

0:13:52.889,0:13:56.929
Неймовірно, що будь-яка мережа робить це добре на зображеннях, яких раніше ніколи не бачила

0:13:57.389,0:14:00.919
Враховуючи, що ми ніколи конкретно не говорили, на які шаблони слід звернути увагу

0:14:02.579,0:14:07.068
Спочатку таким чином я мотивував цю структуру, описуючи надію, яку ми могли б мати

0:14:07.259,0:14:09.739
Щоб другий шар міг забратись на невеликих краях

0:14:09.809,0:14:17.089
Щоб третій шар з'єднав ці краї, щоб розпізнати петлі та довші лінії, і щоб вони могли бути зібрані разом для розпізнавання цифр

0:14:17.699,0:14:22.729
Так це насправді робить наша мережа? Ну для цього хоча б

0:14:23.339,0:14:24.449
Зовсім ні

0:14:24.449,0:14:27.409
пригадайте, як в минулому відео ми дивилися, як ваги

0:14:27.480,0:14:31.849
З'єднання від усіх нейронів першого шару до заданого нейрона у другому шарі

0:14:31.980,0:14:36.829
Можна візуалізувати як заданий піксельний малюнок, на якому піднімається нейрон другого шару

0:14:37.350,0:14:43.309
Добре, коли ми насправді робимо це для ваг, пов'язаних із цими переходами від першого шару до іншого

0:14:43.709,0:14:50.209
Замість того, щоб збирати на окремих маленьких краях тут і там. Вони добре виглядають майже випадково

0:14:50.370,0:14:56.399
Просто помістіть кілька дуже вільних візерунків посередині, здавалося б, що в незбагненно великих

0:14:56.920,0:15:02.580
13000 мірних просторів можливих ваг і ухилів наша мережа виявила себе маленьким задоволеним місцевим мінімумом

0:15:02.860,0:15:08.940
незважаючи на успішну класифікацію більшості зображень, точно не підходить до тих шаблонів, на які ми могли б сподіватися та

0:15:09.430,0:15:13.709
Щоб дійсно загнати цю точку додому, дивіться, що відбувається при введенні випадкового зображення

0:15:14.019,0:15:21.449
якщо система була розумною, ви можете очікувати, що вона відчує себе невпевнено, можливо, насправді не активує жоден з цих 10 вихідних нейронів або

0:15:21.579,0:15:23.200
Активізуйте їх все рівномірно

0:15:23.200,0:15:24.820
Але замість цього

0:15:24.820,0:15:32.010
Впевнено дає вам якусь дурницьку відповідь, ніби він відчуває себе так само впевненим, що цей випадковий шум - це 5, як це фактично

0:15:32.010,0:15:34.010
зображення 5 - це 5

0:15:34.180,0:15:40.829
фраза по-різному, навіть якщо ця мережа може розпізнати цифри досить добре, вона не має уявлення, як їх намалювати

0:15:41.500,0:15:45.149
Багато цього в тому, що це така жорстка обмежена програма тренувань

0:15:45.149,0:15:51.479
Я маю на увазі поставити себе в мережу взуття тут, з її точки зору, весь Всесвіт складається ні з чого

0:15:51.480,0:15:57.539
Але чітко визначені незмінні цифри, зосереджені в крихітній сітці, і її функція витрат просто ніколи не давала

0:15:57.700,0:16:00.959
Заохочує бути будь-чим, але повністю впевнений у своїх рішеннях

0:16:01.690,0:16:05.070
Тож якщо це зображення того, чим реально займаються ці нейрони другого шару

0:16:05.140,0:16:09.839
Ви можете задатися питанням, чому я б познайомив цю мережу з мотивацією підбирати по краях та шаблонах

0:16:09.839,0:16:11.969
Я маю на увазі, що це зовсім не те, що в кінцевому підсумку робить

0:16:13.029,0:16:17.909
Ну, це означає не нашу кінцеву мету, а навпаки, відверту точку відправлення

0:16:17.910,0:16:19.120
Це стара технологія

0:16:19.120,0:16:21.510
вид, що досліджувався у 80-х та 90-х та

0:16:21.640,0:16:29.129
Вам потрібно зрозуміти це, перш ніж ви зможете зрозуміти більш детальні сучасні варіанти, і це однозначно здатне вирішити деякі цікаві проблеми

0:16:29.410,0:16:34.110
Але чим більше ви копаєтеся до того, що ці приховані шари справді роблять, тим менш розумними здаються

0:16:38.530,0:16:42.359
На мить зміщуючи фокус від того, як навчаються мережі, як ви навчаєтесь

0:16:42.580,0:16:46.139
Це станеться лише в тому випадку, якщо ви якось активно залучаєте сюди матеріал

0:16:46.660,0:16:53.100
Я хочу зробити так, щоб ви просто зробили паузу і на хвилину задумалися над тим, що робити

0:16:53.440,0:16:55.230
Зміни, які ви можете внести в цю систему

0:16:55.230,0:17:00.719
І як це сприймає зображення, якщо ви хотіли, щоб він краще підбирав такі речі, як краї та візерунки?

0:17:01.360,0:17:04.410
Але краще, ніж насправді займатися матеріалом

0:17:04.410,0:17:05.079
Я

0:17:05.079,0:17:08.969
Дуже рекомендую книгу Майкла Нільсена про глибоке навчання та нейронні мережі

0:17:09.190,0:17:14.369
У ньому ви можете знайти код і дані, які потрібно завантажити та грати з цим точним прикладом

0:17:14.410,0:17:18.089
І книга буде переглядати вас крок за кроком, що робить цей код

0:17:18.910,0:17:21.749
Дивовижним є те, що ця книга є безкоштовною та загальнодоступною

0:17:22.360,0:17:27.540
Тож якщо у вас щось виходить, подумайте про приєднання до мене пожертви на зусилля Нільсена

0:17:27.910,0:17:32.219
Я також пов'язав кілька інших ресурсів, які мені дуже подобаються в описі, включаючи

0:17:32.470,0:17:36.390
феноменальна і красива публікація в блозі Кріса Ола та статті в спирті

0:17:38.230,0:17:40.200
Щоб закрити тут речі протягом останніх кількох хвилин

0:17:40.200,0:17:43.740
Я хочу перестрибнути назад у фрагмент інтерв'ю, яке було у мене з Лейшею Лі

0:17:43.930,0:17:49.079
Ви можете згадати її з останнього відео. Вона робила свою докторську роботу в глибокому навчанні та в цьому маленькому фрагменті

0:17:49.080,0:17:55.530
Вона розповідає про два останні документи, які справді вивчають, як насправді навчаються деякі сучасніші мережі розпізнавання зображень

0:17:55.810,0:18:01.349
Тільки для того, щоб встановити, де ми були в розмові, перший документ взяв одну з таких особливо глибоких нейронних мереж

0:18:01.350,0:18:05.910
Це дійсно добре в розпізнаванні зображень і замість того, щоб тренувати його на належним чином маркованих даних

0:18:05.910,0:18:08.579
Встановіть перетасовувати всі мітки перед тренуванням

0:18:08.800,0:18:14.669
Очевидно, що точність тестування тут буде не кращою, ніж випадкова, оскільки все позначено випадковим чином

0:18:14.800,0:18:20.879
Але все-таки вдалося досягти такої ж точності тренувань, що і ви на належній мітці даних

0:18:21.490,0:18:27.540
По суті, мільйонів ваг для цієї конкретної мережі було достатньо для того, щоб просто запам'ятати випадкові дані

0:18:27.820,0:18:34.379
Який вид викликає питання про те, чи мінімізація цієї функції витрат насправді відповідає будь-якій структурі зображення?

0:18:34.380,0:18:36.380
Або це просто ти знаєш?

0:18:36.520,0:18:37.420
запам’ятати ціле

0:18:37.420,0:18:43.859
Набір даних про те, що таке правильна класифікація, і тому вас пару знають через півроку в ICML цього року

0:18:44.470,0:18:49.039
Не було точно паперового паперу для спростування, який би звертався до деяких запитань, як ей

0:18:49.470,0:18:55.279
Насправді ці мережі роблять щось трохи розумніше, ніж це, якщо подивитися на цю криву точності

0:18:55.279,0:18:57.499
якби ви тільки тренувалися на а

0:18:58.259,0:19:05.179
Випадковий набір даних, що крива на зразок знизився, ви дуже повільно знаєте майже лінійно

0:19:05.179,0:19:09.589
Таким чином, ви справді намагаєтеся знайти локальні мінімуми

0:19:09.590,0:19:15.289
ви знаєте правильні ваги, які отримали б вам таку точність, тоді як якщо ви насправді тренуєтесь на структурованому наборі даних, який має такий

0:19:15.289,0:19:21.439
Праві етикетки. Ви знаєте, що на початку трохи заплутуєтесь, але потім ви дуже швидко впали, щоб дійти до цього

0:19:22.200,0:19:26.149
Рівень точності, і тому в деякому сенсі це було легше знайти

0:19:26.759,0:19:33.949
Місцеві максимуми, і це також було цікаво про те, що це спіймане виводить на світ ще один папір фактично пару років тому

0:19:34.080,0:19:36.080
Що має набагато більше

0:19:36.990,0:19:39.169
спрощення щодо мережевих шарів

0:19:39.169,0:19:46.788
Але одним з результатів було те, що якщо подивитися на оптимізаційний ландшафт, то локальні мінімуми, які ці мережі, як правило, вивчають

0:19:47.340,0:19:54.079
Насправді однакової якості, тому в певному сенсі, якщо ваш набір даних є структурою, і ви зможете знайти це набагато простіше

0:19:58.139,0:20:01.189
Моя подяка, як завжди, тим, хто підтримує патреон

0:20:01.190,0:20:06.950
Я раніше говорив лише про те, що таке "патреон", що змінить ігри, але без них я справді не був би можливим

0:20:07.230,0:20:12.889
Також хочуть дати спеціальну. Завдяки партнерам фірми VC amplifi за підтримку цих початкових роликів у цій серії

0:20:13.470,0:20:17.149
Вони зосереджуються на машинному навчанні на дуже ранній стадії та AI компаніях

0:20:17.309,0:20:19.110
і я відчуваю себе досить впевнено

0:20:19.110,0:20:23.899
Ймовірність того, що хтось із вас спостерігає за цим, а ще ймовірніше, що це хтось із людей, яких ви знаєте

0:20:24.210,0:20:27.949
зараз на ранніх стадіях виведення такої компанії з місця та

0:20:28.230,0:20:31.279
Ампліфіки люблять почути від будь-яких таких засновників

0:20:31.279,0:20:37.069
і вони навіть встановили адресу електронної пошти саме для цього відео, щоб ви могли зв’язатися з ними через три синіх, один коричневий на

0:20:37.590,0:20:39.590
посилити партнерів ком
